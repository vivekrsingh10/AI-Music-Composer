# AI Music Composer üéπ
<p align="center">
  <img src="https://github.com/user-attachments/assets/c316b03c-f11e-45f5-95b9-e640e3319c88" alt="Project Img" width="575">
</p>

Link to my project: https://huggingface.co/spaces/vksingh7532/AI-Music-Composer

This project uses a Long Short-Term Memory (LSTM) neural network to learn the patterns, melodies, and harmonies from a dataset of classical piano music. It can then generate new, original compositions in the same style.



---
## Key Features

* **Music Generation:** Creates novel piano compositions in MIDI format.
* **Style Learning:** The model is trained on a corpus of classical music to learn its underlying structure.
* **LSTM Architecture:** Built with a deep LSTM network using TensorFlow and Keras, ideal for learning sequential patterns.
* **Customizable:** Can be re-trained on any collection of MIDI files to learn different musical styles.
* **Analysis Tools:** Includes a script to programmatically compare the differences between generated compositions.

---
## Tech Stack

* **Python 3.9+**
* **TensorFlow / Keras** for building and training the LSTM model.
* **Music21** for parsing and manipulating MIDI files.
* **NumPy** for numerical operations.
* **Google Colab** for GPU-accelerated training.
* **Flask & Gunicorn** for the web application backend.
* **Hugging Face Spaces** for deployment.

---
## Setup and Installation

To run this project locally, follow these steps:

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/vivekrsingh10/AI-Music-Composer.git](https://github.com/vivekrsingh10/AI-Music-Composer.git)
    cd AI-Music-Composer
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv myenv
    source myenv/bin/activate  # On Windows, use `myenv\Scripts\activate`
    ```

3.  **Install dependencies:**
    *(Note: You'll need to create a `requirements.txt` file containing `tensorflow`, `music21`, and `numpy`)*
    ```bash
    pip install -r requirements.txt
    ```

4.  **Download the Training Data:**
    The model was trained on the "Classical Music MIDI" dataset from Kaggle.
    * Download the dataset from [this link](https://www.kaggle.com/datasets/soumikrakshit/classical-music-midi).
    * Create a folder named `midi_files` in the project directory.
    * Place all the `.mid` files from the dataset into the `midi_files` folder.

---
## Usage

The project is structured within the main Colab notebook (`AI_Music_Composer.ipynb`) or a Python script.

### Generating Music (Using the Pre-trained Model)

The repository includes a pre-trained model (`music_model.h5`) and the necessary data file (`data/notes`). To generate a new piece:

1.  Ensure the `music_model.h5` and `data/notes` files are present.
2.  Open the notebook or script and call the `generate()` function. A new, unique MIDI file will be created in the root directory.

### Training a New Model

To train the model from scratch on your own dataset:

1.  Make sure your custom MIDI files are in the `midi_files` folder.
2.  Call the `train_network()` function. This process is computationally intensive and is best run in a GPU-accelerated environment like Google Colab.
3.  A new `music_model.h5` and `data/notes` file will be generated upon completion.

---
## Sample Output

Listen to a sample composition generated by this model:

‚ñ∂Ô∏è **[sample_output.mid](https://github.com/vivekrsingh10/AI-Music-Composer/blob/main/sample_output_1.mid)**
‚ñ∂Ô∏è **[sample_output.mid](https://github.com/vivekrsingh10/AI-Music-Composer/blob/main/sample_output_2.mid)**

---
## Future Improvements

* **Implement a Transformer Model:** Explore using a more modern Transformer architecture with attention mechanisms for potentially more complex and long-range musical structures.
* **Add More Musical Nuance:** Extend the model to learn and predict note **duration** and **velocity** (volume) for more expressive compositions.
* **Improve Web App UI:** Enhance the user interface, possibly using a framework like Gradio or Streamlit, to allow for more user interaction, such as selecting a "priming" note to start the composition.

---
## License

This project is licensed under the MIT License - see the `LICENSE` file for details.
